# My Bonafide Work Experience and Externship Projects I have undertaken
My Portfolio of all the projects I did for both my Stackfolio's Data Engineer and ML Engineering Projects.

## [Tiger Analytics ETL Pipeline Project.](https://github.com/17mihir/Stackfolio-s-Data-Engineering-Captsone-Project.git)
![Alt Text](https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.databricks.com%2Fsites%2Fdefault%2Ffiles%2F2020%2F08%2Fblog-accelerate-etl-1-og.png&tbnid=VEs1lVDfU6tjqM&vet=12ahUKEwi3pYLOr7X_AhXKkNgFHdS4DPIQMygDegUIARDCAQ..i&imgrefurl=https%3A%2F%2Fwww.databricks.com%2Fblog%2F2020%2F08%2F18%2Fhow-to-accelerate-your-etl-pipelines-from-18-hours-to-as-fast-as-5-minutes-with-azure-databricks.html&docid=qBpi9Csd3X09jM&w=1200&h=630&q=data%20etl%20pipeline%20architecture%20diagram&ved=2ahUKEwi3pYLOr7X_AhXKkNgFHdS4DPIQMygDegUIARDCAQ)


This project aims to build an ETL pipeline to provide temperature, population, and immigration statistics for different cities. It involves extracting data from multiple datasets, transforming it with Apache Spark, and converting it into JSON files. The JSON files are then uploaded to a Redshift database via Apache Airflow and S3. Further transformations and loading occur in normalized fact and dimension tables using reusable tasks. Data checks are performed to ensure data accuracy and integrity


## [Shiprocket Data Pipline Project with Apache Airflow](https://github.com/17mihir/Stackfolio-s-Sparkify-Data-Pipeline-Project-with-Apache-Airflow.git)

The purpose of this project was to build a dynamic ETL data pipeline that utilizes automation and monitoring. The data pipeline is built from reusable tasks allows for easy backfills. It utilizes custom operators to perform tasks such as staging the data, filling the data warehouse, and running a check on the data as the final step so as to to catch any discrepancies in the datasets.

## [Juspay Data Warehouse Project](https://github.com/17mihir/Sparkify-Data-Warehouse-Project-for-song-play-analysis.git)

The purpose of this project is to build an ETL pipeline that will be able to extract song data from an S3 bucket and transform that data to make it suitable for analysis. This data can be used with business intelligence and visualization apps that will help the analytics team to better understand what songs are commonly listened to on the app.

## [Spotify Data Lake Project with Apache Spark](https://github.com/17mihir/Sparkify-Data-Lake-Project-with-Apache-Spark.git)

The purpose of this project was to build an ETL pipeline that will be able to extract song and log data from an S3 bucket, process the data using Spark and load the data back into s3 as a set of dimensional tables in spark parquet files. This helps analysts to continue finding insights on what their users are listening to.

## [Tredence Spark Streaming Project](https://github.com/17mihir/Spark-Streaming-Project.git)

The purpose of this project was to create a Kafka server to produce data and ingest data through Spark Structured Streaming.
## [Chicago Transit Authority Public Transit Status with Apache Kafka](https://github.com/17mihir/Public-Transit-Status-with-Apache-Kafka.git)

In this project, I constructed a streaming event pipeline around Apache Kafka and its ecosystem. Using public data from the Chicago Transit Authority I built an event pipeline around Kafka that allows me to simulate and display the status of train lines in real time.
